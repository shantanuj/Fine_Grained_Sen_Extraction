{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''What all do i need in the code:\n",
    "For the simple structure:\n",
    "1) Word embedding layer, feeed input layer\n",
    "    1) Creating the gen batch function-> Seq and padded sequence\n",
    "    \n",
    "    \n",
    "    a) Feed input layer: Seq length and padded sequence\n",
    "    b) Labels: Padded too and >>>>>>CHUNKS<<<<<<\n",
    "    c) Word embedding according to vocab loaded \n",
    "    \n",
    "2)DONE  Trim vocab and word embeddings based on vocab in both domains. DONE\n",
    "\n",
    "3) Run through RNN\n",
    "    a) Ideally, have function here to append seq2seq and tree kernel representation\n",
    "    b) Computing tree kernel and gaining a good represnation out of it\n",
    "    \n",
    "4) Decoding: \n",
    "    a) TRAINING:  <F1 measure< Allow for both with and without\n",
    "    b) PREDICTIONS:\n",
    "    1) F1 measure\n",
    "    2) Allow for with and without crf\n",
    "    3) Store model and allow for training continuation\n",
    "    4) Predictions with crf code\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Create joint vocab'''\n",
    "def get_min_vocab(domain_vocabs_list):\n",
    "    min_vocab_size = len(domain_vocabs_list[0])\n",
    "    for domain_vocab in domain_vocabs_list:\n",
    "        if(len(domain_vocab)<min_vocab_size):\n",
    "            smallest_vocab = domain_vocab\n",
    "            min_vocab_size = len(smallest_vocab)\n",
    "    return smallest_vocab\n",
    "\n",
    "def get_common_words_across_domains(domain_vocabs_list):\n",
    "    #Simple optimum -> Start with domain with smallest vocab, and then keep intersecting-> counting freq will be a costlier operation\n",
    "    smallest_vocab = get_min_vocab(domain_vocabs_list)\n",
    "    f_set = set(smallest_vocab.keys())\n",
    "    for domain_vocab in domain_vocabs_list:\n",
    "        set1 = set(domain_vocab.keys())\n",
    "        f_set = f_set.intersection(set1)\n",
    "    \n",
    "    return f_set\n",
    "\n",
    "def get_most_prominent_common_words(domain_names, domain_freqs_list, remove_stop_words = False):\n",
    "    '''Input: MAKE SURE THAT domains_names is in order of domain_freqs_list\n",
    "    Output: Dictionary of words ordered by counts in domains and cumulative freq\n",
    "    To obtain format: Key [\"<ORDER_OF_DOMAINS>\"] \n",
    "    '''\n",
    "    assert len(domain_names) == len(domain_freqs_list) #Just a small limited check\n",
    "    common_word_dict = {}\n",
    "    common_words = get_common_words_across_domains(domain_freqs_list)\n",
    "    common_word_dict[\"<ORDER_OF_DOMAINS>\"] = domain_names+[\"Cumulative_count\"]\n",
    "    for word in common_words:\n",
    "        if(remove_stop_words and word in stop_words):\n",
    "            continue\n",
    "        counts = []\n",
    "        for domain_freq_dict in domain_freqs_list:\n",
    "            counts.append(domain_freq_dict[word])\n",
    "        counts.append(reduce(lambda x,y:x+y, counts))\n",
    "        common_word_dict[word] = counts\n",
    "    \n",
    "    return common_word_dict\n",
    "\n",
    "def get_domain_vocab_and_freq(domain_names):\n",
    "    vocab_freq_list = []\n",
    "    for domain in domains:\n",
    "        with open(domain_vfreq_dir.format(domain, domain)) as p1:\n",
    "            vocab_freq_list.append(pickle.load(p1))\n",
    "    return vocab_freq_list\n",
    "\n",
    "'''Create joint vocab'''\n",
    "def get_joint_vocab(domains, domain_general_path = \"./Final_joint_data/Domains/{}/\",output_path = \"./Final_joint_data/\"):\n",
    "    output_pkl_vocab = output_path+'vocab_to_id.pkl'\n",
    "    output_pkl_vocab_freq = output_path+'vocab_to_freq.pkl'\n",
    "    tr_data_pkl = domain_general_path+\"Normal__normal_training_list.pickle\"\n",
    "    \n",
    "    vocab2id = {}\n",
    "    vocab2freq = {}\n",
    "    for domain in domains:\n",
    "        with open(tr_data_pkl.format(domain),'r') as p1:\n",
    "            tr_data = pickle.load(p1)\n",
    "            for row in tr_data: #go through each row\n",
    "                for word in row[0]:\n",
    "                    if(word in vocab2id.keys()):\n",
    "                        vocab2freq[word]+=1\n",
    "                    else:\n",
    "                        vocab2id[word] = len(vocab2id)\n",
    "                        vocab2freq[word] = 1 \n",
    "    vocab2id['<UNK>'] = len(vocab2id)\n",
    "    vocab2id['<START>'] = len(vocab2id)\n",
    "    vocab2id['<END>'] = len(vocab2id)\n",
    "    with open(output_pkl_vocab,'w') as p1:\n",
    "        pickle.dump(vocab2id,p1)\n",
    "    with open(output_pkl_vocab_freq,'w') as p1:\n",
    "        pickle.dump(vocab2freq,p1)\n",
    "    \n",
    "    return vocab2id, vocab2freq\n",
    "    \n",
    "vocab1, vocab2 = get_joint_vocab([\"Laptop\",\"Rest\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = Word2Vec.load('./Final_joint_data/Embeddings/Gensim_w2vec/laptop_200d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x.wv.save_word2vec_format(\"./Final_joint_data/Embeddings/Gensim_w2vec/only_laptop_200d.txt\")\n",
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-dccb80cc61d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"food\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x.most_similar(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-323d446dd133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mword_embedding_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mvocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mn_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0muse_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "def trim_word_vectors(input_path, output_path, vocab_path, dim=200, use_glove_format = True):\n",
    "    #Go through each line of embeddings\n",
    "    #if in vocab then add \n",
    "    #Outputs vectors in npy format allowing for direct loading in model\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    \n",
    "    print(\"Reading word vectors from file\")\n",
    "    with open(input_path) as f:\n",
    "        if(not use_glove_format):\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if(word in vocab):\n",
    "                word_id = vocab[word]\n",
    "                embeddings[word_id] = np.asarray(embedding)\n",
    "    np.savez_compressed(output_path, embeddings = embeddings)\n",
    "    print(\"Saved trimmed word vectors in np compressed format at: {}\".format(output_path))\n",
    "    \n",
    "if(__name__ == \"__main__\"):\n",
    "    word_embedding_path = sys.argv[1]\n",
    "    output_path = sys.argv[2]\n",
    "    vocab_path = sys.argv[3]\n",
    "    n_dims = sys.argv[4]\n",
    "    use_glove = bool(int(sys.argv[5]))\n",
    "    trim_word_vectors(word_embedding_path, output_path, vocab_path, n_dims, use_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trim_word_vectors(input_path, output_path, vocab_path, dim=200, use_glove_format = True):\n",
    "    #Go through each line of embeddings\n",
    "    #if in vocab then add \n",
    "    #Outputs vectors in npy format allowing for direct loading in model\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    embeddings = np.zeros([len(vocab), dim])\n",
    "    \n",
    "    print(\"Reading word vectors from file\")\n",
    "    with open(input_path) as f:\n",
    "        if(not use_glove_format):\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            line = line.strip().split(' ')\n",
    "            word = line[0]\n",
    "            embedding = [float(x) for x in line[1:]]\n",
    "            if(word in vocab):\n",
    "                word_id = vocab[word]\n",
    "                embeddings[word_id] = np.asarray(embedding)\n",
    "    np.savez_compressed(output_path, embeddings = embeddings)\n",
    "    print(\"Saved trimmed word vectors in np compressed format at: {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading word vectors from file\n",
      "Saved compressed word vectors at: np_Togetherw2vec_200d_trimmed\n"
     ]
    }
   ],
   "source": [
    "trim_word_vectors(\"./Final_joint_data/Embeddings/together_200d.txt\", \"np_Togetherw2vec_200d_trimmed\",\"Final_joint_data/vocab_to_id.pkl\", 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tr_data_pkl = \"./Final_joint_data/Domains/Rest/Normal__normal_training_list.pickle\"\n",
    "with open(tr_data_pkl, 'r') as p1:\n",
    "    tr_data = pickle.load(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['but', 'the', 'staff', 'was', 'so', 'horrible', 'to', 'us', ' <PERIOD> '],\n",
       " [5, 5, 1, 5, 5, 3, 5, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3044"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('./Final_joint_data/tag2id.pkl','r') as p1:\n",
    "    x = pickle.load(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<END>': -1, '<START>': 0, 'B-A': 1, 'B-O': 3, 'I-A': 2, 'I-O': 4, 'O': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab1, vocab2 = get_joint_vocab([\"Laptop\",\"Rest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def trimmed_vectors(input_word_vector_file, input_vocab):\n",
    "    output_path = input_word_vector_file + \"trimmed.txt\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
