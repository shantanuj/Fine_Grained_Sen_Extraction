{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''What all do i need in the code:\n",
    "For the simple structure:\n",
    "\n",
    "\n",
    "1) Word embedding layer, feeed input layer\n",
    "    1) Creating the gen batch function-> Seq and padded sequence\n",
    "    a) Feed input layer: Seq length and padded sequence\n",
    "    b) Labels: Padded too and >>>>>>CHUNKS<<<<<<\n",
    "    c) Word embedding according to vocab loaded \n",
    "    \n",
    "2)DONE  Trim vocab and word embeddings based on vocab in both domains. DONE\n",
    "\n",
    "3) Run through RNN\n",
    "    a) Ideally, have function here to append seq2seq and tree kernel representation\n",
    "    b) Computing tree kernel and gaining a good represnation out of it\n",
    "    \n",
    "4) Decoding: \n",
    "    a) TRAINING:  <F1 measure< Allow for both with and without\n",
    "    b) PREDICTIONS:\n",
    "    1) F1 measure\n",
    "    2) Allow for with and without crf and without seq2seq\n",
    "    3) STORE model and allow for training continuation\n",
    "    4) Predictions with crf code\n",
    "    5) STORE accuracy in a file\n",
    "    \n",
    "    6)Integrate seq2seq\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./sequence_tagging/data/vocab_to_id.pkl','r') as p1:\n",
    "    x = pickle.load(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7053"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from .data_utils import minibatches, pad_sequences, get_chunks\n",
    "from .general_utils import Progbar\n",
    "from .base_model import BaseModel\n",
    "\n",
    "\n",
    "class NERModel(BaseModel):\n",
    "    \"\"\"Specialized class of Model for NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(NERModel, self).__init__(config)\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Define placeholders = entries to computational graph\"\"\"\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_ids\")\n",
    "\n",
    "        # shape = (batch size)\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],\n",
    "                        name=\"sequence_lengths\")\n",
    "\n",
    "        # shape = (batch size, max length of sentence, max length of word)\n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None],\n",
    "                        name=\"char_ids\")\n",
    "\n",
    "        # shape = (batch_size, max_length of sentence)\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"word_lengths\")\n",
    "\n",
    "        # shape = (batch size, max length of sentence in batch)\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None],\n",
    "                        name=\"labels\")\n",
    "\n",
    "        # hyper parameters\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"dropout\")\n",
    "        self.lr = tf.placeholder(dtype=tf.float32, shape=[],\n",
    "                        name=\"lr\")\n",
    "\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "        if self.config.use_chars:\n",
    "            char_ids, word_ids = zip(*words)\n",
    "            word_ids, sequence_lengths = pad_sequences(word_ids, 0)\n",
    "            char_ids, word_lengths = pad_sequences(char_ids, pad_tok=0,\n",
    "                nlevels=2)\n",
    "        else:\n",
    "            word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "\n",
    "        if self.config.use_chars:\n",
    "            feed[self.char_ids] = char_ids\n",
    "            feed[self.word_lengths] = word_lengths\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, sequence_lengths\n",
    "\n",
    "\n",
    "    def add_word_embeddings_op(self):\n",
    "        \"\"\"Defines self.word_embeddings\n",
    "\n",
    "        If self.config.embeddings is not None and is a np array initialized\n",
    "        with pre-trained word vectors, the word embeddings is just a look-up\n",
    "        and we don't train the vectors. Otherwise, a random matrix with\n",
    "        the correct shape is initialized.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            if self.config.embeddings is None:\n",
    "                self.logger.info(\"WARNING: randomly initializing word vectors\")\n",
    "                _word_embeddings = tf.get_variable(\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        shape=[self.config.nwords, self.config.dim_word])\n",
    "            else:\n",
    "                _word_embeddings = tf.Variable(\n",
    "                        self.config.embeddings,\n",
    "                        name=\"_word_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        trainable=self.config.train_embeddings)\n",
    "\n",
    "            word_embeddings = tf.nn.embedding_lookup(_word_embeddings,\n",
    "                    self.word_ids, name=\"word_embeddings\")\n",
    "\n",
    "        with tf.variable_scope(\"chars\"):\n",
    "            if self.config.use_chars:\n",
    "                # get char embeddings matrix\n",
    "                _char_embeddings = tf.get_variable(\n",
    "                        name=\"_char_embeddings\",\n",
    "                        dtype=tf.float32,\n",
    "                        shape=[self.config.nchars, self.config.dim_char])\n",
    "                char_embeddings = tf.nn.embedding_lookup(_char_embeddings,\n",
    "                        self.char_ids, name=\"char_embeddings\")\n",
    "\n",
    "                # put the time dimension on axis=1\n",
    "                s = tf.shape(char_embeddings)\n",
    "                char_embeddings = tf.reshape(char_embeddings,\n",
    "                        shape=[s[0]*s[1], s[-2], self.config.dim_char])\n",
    "                word_lengths = tf.reshape(self.word_lengths, shape=[s[0]*s[1]])\n",
    "\n",
    "                # bi lstm on chars\n",
    "                cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_char,\n",
    "                        state_is_tuple=True)\n",
    "                _output = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        cell_fw, cell_bw, char_embeddings,\n",
    "                        sequence_length=word_lengths, dtype=tf.float32)\n",
    "\n",
    "                # read and concat output\n",
    "                _, ((_, output_fw), (_, output_bw)) = _output\n",
    "                output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "                # shape = (batch size, max sentence length, char hidden size)\n",
    "                output = tf.reshape(output,\n",
    "                        shape=[s[0], s[1], 2*self.config.hidden_size_char])\n",
    "                word_embeddings = tf.concat([word_embeddings, output], axis=-1)\n",
    "\n",
    "        self.word_embeddings =  tf.nn.dropout(word_embeddings, self.dropout)\n",
    "\n",
    "\n",
    "    def add_logits_op(self):\n",
    "        \"\"\"Defines self.logits\n",
    "\n",
    "        For each word in each sentence of the batch, it corresponds to a vector\n",
    "        of scores, of dimension equal to the number of tags.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw, cell_bw, self.word_embeddings,\n",
    "                    sequence_length=self.sequence_lengths, dtype=tf.float32)\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.nn.dropout(output, self.dropout)\n",
    "\n",
    "        with tf.variable_scope(\"proj\"):\n",
    "            W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                    shape=[2*self.config.hidden_size_lstm, self.config.ntags])\n",
    "\n",
    "            b = tf.get_variable(\"b\", shape=[self.config.ntags],\n",
    "                    dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "            nsteps = tf.shape(output)[1]\n",
    "            output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "            pred = tf.matmul(output, W) + b\n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags])\n",
    "\n",
    "\n",
    "    def add_pred_op(self):\n",
    "        \"\"\"Defines self.labels_pred\n",
    "\n",
    "        This op is defined only in the case where we don't use a CRF since in\n",
    "        that case we can make the prediction \"in the graph\" (thanks to tf\n",
    "        functions in other words). With theCRF, as the inference is coded\n",
    "        in python and not in pure tensroflow, we have to make the prediciton\n",
    "        outside the graph.\n",
    "        \"\"\"\n",
    "        if not self.config.use_crf:\n",
    "            self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1),\n",
    "                    tf.int32)\n",
    "\n",
    "\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"Defines the loss\"\"\"\n",
    "        if self.config.use_crf:\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(\n",
    "                    self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "        else:\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits, labels=self.labels)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        # for tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "\n",
    "    def build(self):\n",
    "        # NER specific functions\n",
    "        self.add_placeholders()\n",
    "        self.add_word_embeddings_op()\n",
    "        self.add_logits_op()\n",
    "        self.add_pred_op()\n",
    "        self.add_loss_op()\n",
    "\n",
    "        # Generic functions that add training op and initialize session\n",
    "        self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "                self.config.clip)\n",
    "        self.initialize_session() # now self.sess is defined and vars are init\n",
    "\n",
    "\n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        if self.config.use_crf:\n",
    "            # get tag scores and transition params of CRF\n",
    "            viterbi_sequences = []\n",
    "            logits, trans_params = self.sess.run(\n",
    "                    [self.logits, self.trans_params], feed_dict=fd)\n",
    "\n",
    "            # iterate over the sentences because no batching in vitervi_decode\n",
    "            for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "                logit = logit[:sequence_length] # keep only the valid steps\n",
    "                viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                        logit, trans_params)\n",
    "                viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "            return viterbi_sequences, sequence_lengths\n",
    "\n",
    "        else:\n",
    "            labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "            return labels_pred, sequence_lengths\n",
    "\n",
    "\n",
    "    def run_epoch(self, train, dev, epoch):\n",
    "        \"\"\"Performs one complete pass over the train set and evaluate on dev\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of sentences, tags\n",
    "            dev: dataset\n",
    "            epoch: (int) index of the current epoch\n",
    "\n",
    "        Returns:\n",
    "            f1: (python float), score to select model on, higher is better\n",
    "\n",
    "        \"\"\"\n",
    "        # progbar stuff for logging\n",
    "        batch_size = self.config.batch_size\n",
    "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
    "        prog = Progbar(target=nbatches)\n",
    "\n",
    "        # iterate over dataset\n",
    "        for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "            fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                    self.config.dropout)\n",
    "\n",
    "            _, train_loss, summary = self.sess.run(\n",
    "                    [self.train_op, self.loss, self.merged], feed_dict=fd)\n",
    "\n",
    "            prog.update(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            # tensorboard\n",
    "            if i % 10 == 0:\n",
    "                self.file_writer.add_summary(summary, epoch*nbatches + i)\n",
    "\n",
    "        metrics = self.run_evaluate(dev)\n",
    "        msg = \" - \".join([\"{} {:04.2f}\".format(k, v)\n",
    "                for k, v in metrics.items()])\n",
    "        self.logger.info(msg)\n",
    "\n",
    "        return metrics[\"f1\"]\n",
    "\n",
    "    def calculate_f1(self, tp,fp,tn,fn):\n",
    "        recall = float(tp)/(tp+fn)\n",
    "        precision = float(tp)/(tp+fp)\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "        return f1, recall, precision\n",
    "\n",
    "    def run_evaluate(self, test):\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        asp_tp = 0.\n",
    "        asp_fp = 0.\n",
    "        asp_tn = 0.\n",
    "        asp_fn = 0.\n",
    "    \n",
    "        op_tp = 0.\n",
    "        op_fp = 0.\n",
    "        op_tn = 0.\n",
    "        op_fn = 0.\n",
    "    \n",
    "        ot_tp = 0.\n",
    "        ot_fp = 0.\n",
    "        ot_tn = 0.\n",
    "        ot_fn = 0.\n",
    "        \n",
    "        tag2id = self.config.vocab_tags \n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred,\n",
    "                                             sequence_lengths):\n",
    "                lab      = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                \n",
    "                \n",
    "                for actual,pred in zip(lab, lab_pred):\n",
    "                    actual = int(actual)\n",
    "                    pred = int(pred)\n",
    "                    #print(actual, actual ==4)\n",
    "                    #print(pred, pred ==4)\n",
    "                    if(actual == tag2id['B-A'] or actual == tag2id): #BA or IA-> Replace by tag2id later\n",
    "                        if(pred == 0 or pred == 2):\n",
    "                            asp_tp +=1\n",
    "                            op_tn +=1\n",
    "                            ot_tn +=1\n",
    "                        else:\n",
    "                            if(pred == 1 or pred==3): \n",
    "                                asp_fn+=1\n",
    "                                op_fp+=1\n",
    "                                ot_tn+=1\n",
    "                            elif(pred==4):\n",
    "                                asp_fn+=1\n",
    "                                ot_fp+=1\n",
    "                                op_tn+=1\n",
    "                            else:\n",
    "                                print(\"Somethings wrong in prediction\")\n",
    "                            \n",
    "                    elif(actual == 1 or actual == 3): #BO or IO\n",
    "                        if(pred == 1 or pred == 3):\n",
    "                            op_tp +=1\n",
    "                            asp_tn +=1\n",
    "                            ot_tn +=1\n",
    "                        else:\n",
    "                            if(pred == 0 or pred==2): \n",
    "                                op_fn+=1\n",
    "                                asp_fp+=1\n",
    "                                ot_tn+=1\n",
    "                            elif(pred==4):\n",
    "                                op_fn+=1\n",
    "                                ot_fp+=1\n",
    "                                asp_tn+=1\n",
    "                            else:\n",
    "                                print(\"Somethings wrong in prediction\")\n",
    "                                \n",
    "                                \n",
    "                    elif(actual == 4):\n",
    "                        if(pred == 4):\n",
    "                            ot_tp +=1\n",
    "                            asp_tn +=1\n",
    "                            op_tn +=1\n",
    "                        else:\n",
    "                            if(pred == 0 or pred==2): \n",
    "                                ot_fn+=1\n",
    "                                asp_fp+=1\n",
    "                                op_tn+=1\n",
    "                            elif(pred==1 or pred==3):\n",
    "                                ot_fn+=1\n",
    "                                op_fp+=1\n",
    "                                asp_tn+=1\n",
    "                            else:\n",
    "                                print(\"Somethings wrong in prediction\")                                \n",
    "                    else:\n",
    "                        print(\"Somethings wrong\")\n",
    "                   \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                \n",
    "                accs    += [a==b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "                lab_chunks      = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds   += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "        \n",
    "        assert(asp_tp+asp_fp+asp_tn+asp_fn == op_tp+op_fp+op_tn+op_fn == ot_tp+ot_fp+ot_tn+ot_fn)\n",
    "        asp_scores = self.calculate_f1(asp_tp,asp_fp,asp_tn,asp_fn)\n",
    "        op_scores = self.calculate_f1(op_tp,op_fp,op_tn,op_fn)\n",
    "        ot_scores = self.calculate_f1(ot_tp,ot_fp,ot_tn,ot_fn)\n",
    "                \n",
    "                \n",
    "                \n",
    "        p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100*acc, \"f1\": 100*f1, \"asp_f1\":100*asp_scores[0], \"op_f1\":100*op_scores[0], \"ot_f1\":100*ot_scores[0]}\n",
    "\n",
    "\n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(3)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calculate_f1(tp,fp,tn,fn):\n",
    "    print(tp)\n",
    "    recall = float(tp)/(tp+fn)\n",
    "    precision = float(tp)/(tp+fp)\n",
    "    f1 = 2*float((precision*recall))/((precision+recall))\n",
    "    return f1, recall, precision\n",
    "def lenient_metrics_absa(obj):\n",
    "    asp_tp = 0\n",
    "    asp_fp = 0\n",
    "    asp_tn = 0\n",
    "    asp_fn = 0\n",
    "    \n",
    "    op_tp = 0\n",
    "    op_fp = 0\n",
    "    op_tn = 0\n",
    "    op_fn = 0\n",
    "    \n",
    "    ot_tp = 0\n",
    "    ot_fp = 0\n",
    "    ot_tn = 0\n",
    "    ot_fn = 0\n",
    "    for lab, lab_pred, length in obj:\n",
    "        lab = lab[:length]\n",
    "        lab_pred = lab_pred[:length]\n",
    "        for actual,pred in zip(lab, lab_pred):\n",
    "            if(actual == 1 or actual == 2): #BA or IA\n",
    "                if(pred == 1 or pred == 2):\n",
    "                    asp_tp +=1\n",
    "                    op_tn +=1\n",
    "                    ot_tn +=1\n",
    "                else:\n",
    "                    if(pred == 3 or pred==4): \n",
    "                        asp_fn+=1\n",
    "                        op_fp+=1\n",
    "                        ot_tn+=1\n",
    "                    elif(pred == 5):\n",
    "                        asp_fn+=1\n",
    "                        ot_fp+=1\n",
    "                        op_tn+=1\n",
    "                    else:\n",
    "                        print(\"Somethings wrong in prediction\")\n",
    "                        \n",
    "            elif(actual == 3 or actual == 4):#BO or IO\n",
    "                if(pred == 3 or pred==4):\n",
    "                    op_tp +=1\n",
    "                    asp_tn +=1\n",
    "                    ot_tn +=1\n",
    "                else:\n",
    "                    if(pred ==1 or pred ==2):\n",
    "                        op_fn +=1\n",
    "                        asp_fp+=1\n",
    "                        ot_tn+=1\n",
    "                    elif(pred ==5):\n",
    "                        op_fn+=1\n",
    "                        ot_fp+=1\n",
    "                        asp_tn+=1\n",
    "                    else:\n",
    "                        print(\"Somethings wrong in prediction\")\n",
    "            elif(actual == 5): #O\n",
    "                if(pred == 5):\n",
    "                    ot_tp +=1\n",
    "                    asp_tn +=1\n",
    "                    op_tn +=1\n",
    "                else:\n",
    "                    if(pred ==1 or pred ==2):\n",
    "                        ot_fn +=1\n",
    "                        asp_fp+=1\n",
    "                        op_tn+=1\n",
    "                    elif(pred ==2 or pred ==3):\n",
    "                        ot_fn+=1\n",
    "                        op_fp+=1\n",
    "                        asp_tn+=1\n",
    "                    else:\n",
    "                        print(\"Somethings wrong in prediction\")\n",
    "                        \n",
    "            else:\n",
    "                print(\"Somethings wrong\")\n",
    "            \n",
    "    assert(asp_tp+asp_fp+asp_tn+asp_fn == op_tp+op_fp+op_tn+op_fn == ot_tp+ot_fp+ot_tn+ot_fn)\n",
    "    asp_scores = calculate_f1(asp_tp,asp_fp,asp_tn,asp_fn)\n",
    "    op_scores = calculate_f1(op_tp,op_fp,op_tn,op_fn)\n",
    "    ot_scores = calculate_f1(ot_tp,ot_fp,ot_tn,ot_fn)\n",
    "    return asp_scores, op_scores, ot_scores\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels = [[1,2,3,1,2,2,4,5,5],[1,2,3,5,5]]\n",
    "preds = [[1,2,3,1,4,4,4,5,5],[2,1,3,5,5]]\n",
    "length = [10,4]\n",
    "obj = zip(labels, preds, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 2, 3, 1, 2, 2, 4, 5, 5], [1, 2, 3, 1, 4, 4, 4, 5, 5], 10),\n",
       " ([1, 2, 3, 5, 5], [2, 1, 3, 5, 5], 4)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.8333333333333333, 0.7142857142857143, 1.0),\n",
       " (0.7499999999999999, 1.0, 0.6),\n",
       " (1.0, 1.0, 1.0))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenient_metrics_absa(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Focus on creating gen batch function \n",
    "a) Padded seq and seq length; Vocab2id; read file; batch_size, etc\n",
    "b) Step through RNN\n",
    "c) Store a model in tf\n",
    "d) Edit accuracy measure in tf\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
