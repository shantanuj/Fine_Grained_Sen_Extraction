{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3887a96143dd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3887a96143dd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Q1: Should we use a loop with concatenation or a predefined zeros tensor for creation of dropped indices\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q1: Should we use a loop with concatenation or a predefined zeros tensor for creation of dropped indices\n",
    "    \n",
    "\n",
    "# What we want to do:\n",
    "#1) Load two models together--> Loading happens through a session which restores variables, etc. \n",
    "#2) We wish to just load the encoder variables (pretrained) and add them to our computation for composite model\n",
    " \n",
    "\n",
    "#What we try doing is based on the following logic:\n",
    "#1) A graph defines all operations and data flow. \n",
    "#2) However, given that a graph can be invoked through a session, we will need to first create a session. \n",
    "#3) QUes: Can a graph only be invoked through a session\n",
    "\n",
    "#4) Then we load this graph and import the encoder part of it into our composite graph.\n",
    "#5) We finally run a session for this composite graph. \n",
    "\n",
    "\n",
    "\n",
    "#6) HOWEVER, we need to make sure that the encoder is not trained any further. \n",
    "#7) Otherwise, we will continue with just two separate sessions. \n",
    "\n",
    "\n",
    "'''\n",
    "The final procedure:\n",
    "1) Create session to load pretrained graph. Store the graph. Quite session. We do this only once. A graph has to be declared in advance. \n",
    "2) Set the encoder weights to be non trainable. \n",
    "3) Create new session to train original model and use encoder as a part of the operation flow. \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For optimization and graph construction:\n",
    "https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html\n",
    "    \n",
    "#General overview of graphs\n",
    "https://www.tensorflow.org/programmers_guide/graphs#visualizing_your_graph\n",
    "    \n",
    "#Making non trainable variables\n",
    "https://stackoverflow.com/questions/37326002/is-it-possible-to-make-a-trainable-variable-not-trainable?rq=1\n",
    "https://stackoverflow.com/questions/35298326/freeze-some-variables-scopes-in-tensorflow-stop-gradient-vs-passing-variables\n",
    "    \n",
    "#Loading multiple graphs:\n",
    "https://stackoverflow.com/questions/41990014/load-multiple-models-in-tensorflow\n",
    "    \n",
    "#Loading trained weights from one graph to another\n",
    "https://stackoverflow.com/questions/39068703/tensorflow-using-weights-trained-in-one-model-inside-another-different-model?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../model/')\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(filename):\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "        \n",
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domain_name = 'laptop'\n",
    "domain_tr_data_path = '../data/Final_joint_data_absa//Domains/Laptop/Normal__normal_training_list.pickle'\n",
    "embeddings_path = '../data/Embeddings/Pruned/np_glove_200d_trimmed.npz'\n",
    "embeddings_name = 'glove200d'\n",
    "vocab_path = '../data/vocab_to_id.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "word_embeddings_np = get_word_embeddings(embeddings_path)\n",
    "pad_token = '<PAD>' \n",
    "eos_token = '<END>'\n",
    "PAD = vocab[pad_token]\n",
    "EOS = vocab[eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 200\n",
    "encoder_hidden_units = 50 #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data\n",
    "\n",
    "def get_holdout_data(idd_domain_tr_data, num=10):\n",
    "    '''Simple function to check if encoder is functioning properly'''\n",
    "    return idd_domain_tr_data[:num]\n",
    "\n",
    "def batch_modify(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = PAD*np.ones(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths\n",
    "\n",
    "def feed_enc(enc_batch):\n",
    "    \n",
    "    encoder_inputs_, encoder_input_lengths_ = batch_modify(enc_batch)\n",
    "    return {word_ids: encoder_inputs_,\n",
    "            sequence_lengths: encoder_input_lengths_}\n",
    "def feed_enc_train(enc_batch):\n",
    "    encoder_inputs_, encoder_input_lengths_ = batch_modify(enc_batch)\n",
    "    train_y = np.zeros(shape = [len(enc_batch),1])#sequence_lengths.shape[0]\n",
    "    return {word_ids: encoder_inputs_,\n",
    "            sequence_lengths: encoder_input_lengths_,\n",
    "            actual_vals:train_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(shape=(None, None), dtype=tf.int32, name='word_ids')\n",
    "sequence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='sequence_lengths')\n",
    "embeddings = tf.Variable(word_embeddings_np, name=\"word_embeds\",dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''This is just to test again'''\n",
    "\n",
    "\n",
    "with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, word_ids)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length= sequence_lengths,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.stop_gradient(tf.concat([encoder_final_state_c,encoder_final_state_h], 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model_exi\n",
    "embed_type = \"Glove\"\n",
    "model_path = '../results/seq2seq/{}_seq2seqmodel_embeds{}_{}d_{}hiddenunits.ckpt'.format(domain_name,embed_type,input_embedding_size,encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized session\n",
      "loading existing model\n",
      "INFO:tensorflow:Restoring parameters from ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Initialized session\")\n",
    "print(\"loading existing model\")\n",
    "saver.restore(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Cell to test in format for experiment'''\n",
    "def restored_model_enc_out(model_exists_already=True):\n",
    "    f_enc = feed_enc(holdout_data)\n",
    "    encoder_useful_state = sess.run(encoder_concat_rep, f_enc)\n",
    "    return encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idd_data = dataset_load(domain_tr_data_path,vocab_path)\n",
    "holdout_data = get_holdout_data(idd_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder_useful_state = restored_model_enc_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898806 ,  0.5807226 ,  0.49744806, ...,  0.5166379 ,\n",
       "        -0.03120399, -0.18058273],\n",
       "       [-0.4750856 ,  0.721219  ,  0.6492678 , ...,  0.3919891 ,\n",
       "        -0.02343164, -0.18083476],\n",
       "       [-0.3480006 ,  0.43519476,  0.2753141 , ...,  0.19185491,\n",
       "        -0.29673263, -0.18144843],\n",
       "       ...,\n",
       "       [ 0.03478556,  0.4519019 ,  0.0941446 , ...,  0.3134694 ,\n",
       "        -0.06109187,  0.03018548],\n",
       "       [-0.18481615,  0.31060803,  0.45174283, ...,  0.47246057,\n",
       "        -0.11575688, -0.19709298],\n",
       "       [-0.7678361 ,  0.7076941 ,  0.39733076, ...,  0.21106088,\n",
       "        -0.06787533, -0.23257294]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_vals = tf.placeholder(shape=(None, 1),name=\"blabla\", dtype=tf.float32)\n",
    "with tf.variable_scope(\"linear\"):\n",
    "    v1 = tf.Variable(tf.ones([encoder_concat_rep.shape[-1],1]),name=\"v1\")\n",
    "    b1 = tf.Variable(tf.ones([1]), name=\"v2\")\n",
    "    result = tf.add(tf.matmul(encoder_concat_rep,v1),b1)\n",
    "    prediction = tf.tanh(result)\n",
    "    cost = tf.reduce_mean(tf.square(prediction - actual_vals))\n",
    "    training_step = tf.train.GradientDescentOptimizer(20).minimize(cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_restored_model_enc_out(epochs =2,model_exists_already=True):\n",
    "    sess.run(tf.variables_initializer([v1, b1]))\n",
    "    for e_ in range(200):\n",
    "        f_enc = feed_enc_train(holdout_data)\n",
    "        _, l,pr = sess.run([training_step,cost, prediction],f_enc)\n",
    "        print(\"Prediction:{}\".format(pr))\n",
    "        print(\"Loss: {}\".format(l))\n",
    "        encoder_useful_state = sess.run(encoder_concat_rep, f_enc)\n",
    "            \n",
    "        #encoder_useful_state = sess.run(encoder_concat_everything)\n",
    "    return encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:[[0.9999936 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998688698\n",
      "Prediction:[[0.9999934]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.99999356]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998688698\n",
      "Prediction:[[0.99999344]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.9999933]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.9999933 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.9999933]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.9999932 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.9999932]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [1.       ]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.999993 ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.99999297]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.99999315]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.99999297]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998569489\n",
      "Prediction:[[0.99999285]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.999993  ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.99999285]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.9999927]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.9999926 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.9999925]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.99999267]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.99999255]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998450279\n",
      "Prediction:[[0.99999243]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.9999924 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [1.        ]]\n",
      "Loss: 0.999998390675\n",
      "Prediction:[[0.9999923]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999996]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998390675\n",
      "Prediction:[[0.9999923 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998390675\n",
      "Prediction:[[0.99999213]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998390675\n",
      "Prediction:[[0.999992  ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.9999921 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.99999195]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.999992  ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.99999183]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.99999183]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.9999918]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.9999916 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998271465\n",
      "Prediction:[[0.99999154]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999998211861\n",
      "Prediction:[[0.99999154]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998211861\n",
      "Prediction:[[0.9999914 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.0000001 ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998211861\n",
      "Prediction:[[0.9999913 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [1.        ]]\n",
      "Loss: 0.999998211861\n",
      "Prediction:[[0.9999912 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999998211861\n",
      "Prediction:[[0.99999106]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999998092651\n",
      "Prediction:[[0.9999911 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998092651\n",
      "Prediction:[[0.99999094]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999998092651\n",
      "Prediction:[[0.9999908 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997973442\n",
      "Prediction:[[0.9999907 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999998092651\n",
      "Prediction:[[0.99999064]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997973442\n",
      "Prediction:[[0.99999064]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.0000001 ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999998092651\n",
      "Prediction:[[0.9999903 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997973442\n",
      "Prediction:[[0.9999904 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999994]]\n",
      "Loss: 0.999997973442\n",
      "Prediction:[[0.9999902]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999997]\n",
      " [1.       ]\n",
      " [0.9999999]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.9999903]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999998]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.9999901 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.99999   ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.99998975]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.99998987]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997913837\n",
      "Prediction:[[0.9999897 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999994]]\n",
      "Loss: 0.999997794628\n",
      "Prediction:[[0.99998945]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997794628\n",
      "Prediction:[[0.9999893 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999994]]\n",
      "Loss: 0.999997794628\n",
      "Prediction:[[0.99998915]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999997735023\n",
      "Prediction:[[0.9999892 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997794628\n",
      "Prediction:[[0.9999891 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997615814\n",
      "Prediction:[[0.99998885]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997496605\n",
      "Prediction:[[0.9999887 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997496605\n",
      "Prediction:[[0.99998844]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997496605\n",
      "Prediction:[[0.99998844]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997496605\n",
      "Prediction:[[0.9999881 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999994]]\n",
      "Loss: 0.999997496605\n",
      "Prediction:[[0.99998814]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999997437\n",
      "Prediction:[[0.99998796]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997317791\n",
      "Prediction:[[0.9999879 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997437\n",
      "Prediction:[[0.9999878 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997437\n",
      "Prediction:[[0.9999874]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999998]]\n",
      "Loss: 0.999997317791\n",
      "Prediction:[[0.99998724]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997317791\n",
      "Prediction:[[0.999987 ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999997]]\n",
      "Loss: 0.999997258186\n",
      "Prediction:[[0.99998695]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999997258186\n",
      "Prediction:[[0.99998665]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.0000001 ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999997258186\n",
      "Prediction:[[0.9999865]\n",
      " [1.       ]\n",
      " [1.0000001]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999998]]\n",
      "Loss: 0.999997138977\n",
      "Prediction:[[0.99998635]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999997138977\n",
      "Prediction:[[0.99998605]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999997019768\n",
      "Prediction:[[0.999986  ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999976]]\n",
      "Loss: 0.999996960163\n",
      "Prediction:[[0.9999857 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999996960163\n",
      "Prediction:[[0.99998546]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999996960163\n",
      "Prediction:[[0.9999853 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]]\n",
      "Loss: 0.999996960163\n",
      "Prediction:[[0.999985  ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999996840954\n",
      "Prediction:[[0.9999847 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999996840954\n",
      "Prediction:[[0.9999845 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996781349\n",
      "Prediction:[[0.9999841 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996542931\n",
      "Prediction:[[0.99998385]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996542931\n",
      "Prediction:[[0.99998355]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.0000001 ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999996542931\n",
      "Prediction:[[0.9999834 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999996483326\n",
      "Prediction:[[0.999983 ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999998]]\n",
      "Loss: 0.999996364117\n",
      "Prediction:[[0.9999826 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999996304512\n",
      "Prediction:[[0.99998224]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996185303\n",
      "Prediction:[[0.99998206]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999946]\n",
      " [1.        ]\n",
      " [0.9999998 ]]\n",
      "Loss: 0.999996304512\n",
      "Prediction:[[0.9999816 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996066093\n",
      "Prediction:[[0.99998134]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999996006489\n",
      "Prediction:[[0.99998075]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.99999588728\n",
      "Prediction:[[0.9999805]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999996]\n",
      " [1.       ]\n",
      " [0.9999997]]\n",
      "Loss: 0.99999588728\n",
      "Prediction:[[0.99998003]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.9999996 ]]\n",
      "Loss: 0.999995708466\n",
      "Prediction:[[0.9999796 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.9999996 ]]\n",
      "Loss: 0.999995708466\n",
      "Prediction:[[0.9999791]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999995]\n",
      " [1.       ]\n",
      " [0.9999997]]\n",
      "Loss: 0.999995589256\n",
      "Prediction:[[0.9999785 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999995410442\n",
      "Prediction:[[0.999978  ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999996 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999995410442\n",
      "Prediction:[[0.9999776 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.9999997 ]]\n",
      "Loss: 0.999995350838\n",
      "Prediction:[[0.9999768 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.9999996 ]]\n",
      "Loss: 0.999995112419\n",
      "Prediction:[[0.9999762 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.9999996 ]]\n",
      "Loss: 0.999994933605\n",
      "Prediction:[[0.9999756 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.9999996 ]]\n",
      "Loss: 0.999994933605\n",
      "Prediction:[[0.9999751 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999994754791\n",
      "Prediction:[[0.9999743 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999994635582\n",
      "Prediction:[[0.99997365]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999934]\n",
      " [1.        ]\n",
      " [0.99999964]]\n",
      "Loss: 0.999994456768\n",
      "Prediction:[[0.9999729]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [0.9999994]\n",
      " [1.       ]\n",
      " [0.9999995]]\n",
      "Loss: 0.999994397163\n",
      "Prediction:[[0.9999719]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999994]\n",
      " [1.       ]\n",
      " [0.9999995]]\n",
      "Loss: 0.999994158745\n",
      "Prediction:[[0.99997115]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999993 ]\n",
      " [1.        ]\n",
      " [0.9999995 ]]\n",
      "Loss: 0.999993920326\n",
      "Prediction:[[0.9999704]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999993]\n",
      " [1.       ]\n",
      " [0.9999995]]\n",
      "Loss: 0.999993920326\n",
      "Prediction:[[0.99996924]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999994 ]\n",
      " [1.        ]\n",
      " [0.9999995 ]]\n",
      "Loss: 0.999993622303\n",
      "Prediction:[[0.9999683 ]\n",
      " [1.        ]\n",
      " [1.0000001 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999993 ]\n",
      " [1.        ]\n",
      " [0.99999946]]\n",
      "Loss: 0.999993443489\n",
      "Prediction:[[0.99996704]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999992 ]\n",
      " [1.        ]\n",
      " [0.9999995 ]]\n",
      "Loss: 0.999993145466\n",
      "Prediction:[[0.9999659 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999992 ]\n",
      " [1.        ]\n",
      " [0.99999946]]\n",
      "Loss: 0.999992966652\n",
      "Prediction:[[0.9999645 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999993 ]\n",
      " [1.        ]\n",
      " [0.99999934]]\n",
      "Loss: 0.999992549419\n",
      "Prediction:[[0.99996316]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999992 ]\n",
      " [1.        ]\n",
      " [0.9999995 ]]\n",
      "Loss: 0.999992370605\n",
      "Prediction:[[0.9999616]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [0.9999993]\n",
      " [1.       ]\n",
      " [0.9999996]]\n",
      "Loss: 0.999992072582\n",
      "Prediction:[[0.9999601 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.99999917]\n",
      " [1.        ]\n",
      " [0.9999994 ]]\n",
      "Loss: 0.999991714954\n",
      "Prediction:[[0.9999583]\n",
      " [1.       ]\n",
      " [0.9999998]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999993]\n",
      " [1.       ]\n",
      " [0.9999994]]\n",
      "Loss: 0.999991297722\n",
      "Prediction:[[0.9999563 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999917]\n",
      " [1.        ]\n",
      " [0.9999994 ]]\n",
      "Loss: 0.999990940094\n",
      "Prediction:[[0.9999542 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999991 ]\n",
      " [1.        ]\n",
      " [0.99999934]]\n",
      "Loss: 0.999990463257\n",
      "Prediction:[[0.9999517 ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999917]\n",
      " [1.        ]\n",
      " [0.99999934]]\n",
      "Loss: 0.999990105629\n",
      "Prediction:[[0.9999493]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999993]\n",
      " [1.       ]\n",
      " [0.9999994]]\n",
      "Loss: 0.999989628792\n",
      "Prediction:[[0.9999464]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [1.       ]\n",
      " [0.9999999]\n",
      " [0.999999 ]\n",
      " [1.       ]\n",
      " [0.9999993]]\n",
      "Loss: 0.999988913536\n",
      "Prediction:[[0.99994314]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999917]\n",
      " [1.        ]\n",
      " [0.9999993 ]]\n",
      "Loss: 0.999988257885\n",
      "Prediction:[[0.9999394 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999905]\n",
      " [1.        ]\n",
      " [0.9999992 ]]\n",
      "Loss: 0.99998742342\n",
      "Prediction:[[0.9999353 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [0.99999905]\n",
      " [1.        ]\n",
      " [0.9999992 ]]\n",
      "Loss: 0.99998664856\n",
      "Prediction:[[0.9999306 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999989 ]\n",
      " [1.        ]\n",
      " [0.9999993 ]]\n",
      "Loss: 0.999985694885\n",
      "Prediction:[[0.99992514]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999989 ]\n",
      " [1.        ]\n",
      " [0.99999917]]\n",
      "Loss: 0.999984562397\n",
      "Prediction:[[0.9999189 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [0.9999988 ]\n",
      " [1.        ]\n",
      " [0.99999917]]\n",
      "Loss: 0.999983310699\n",
      "Prediction:[[0.9999113 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999987 ]\n",
      " [1.        ]\n",
      " [0.99999905]]\n",
      "Loss: 0.999981760979\n",
      "Prediction:[[0.9999024 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999998 ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999988 ]\n",
      " [1.        ]\n",
      " [0.9999989 ]]\n",
      "Loss: 0.999979972839\n",
      "Prediction:[[0.9998916 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999987 ]\n",
      " [1.        ]\n",
      " [0.9999989 ]]\n",
      "Loss: 0.999977767467\n",
      "Prediction:[[0.99987805]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999986 ]\n",
      " [1.        ]\n",
      " [0.9999987 ]]\n",
      "Loss: 0.999974906445\n",
      "Prediction:[[0.99986076]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999994]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999984 ]\n",
      " [1.        ]\n",
      " [0.9999986 ]]\n",
      "Loss: 0.99997150898\n",
      "Prediction:[[0.9998382 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.9999998 ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.9999998 ]\n",
      " [0.9999983 ]\n",
      " [1.        ]\n",
      " [0.9999985 ]]\n",
      "Loss: 0.999966800213\n",
      "Prediction:[[0.9998071 ]\n",
      " [0.99999994]\n",
      " [0.99999964]\n",
      " [1.        ]\n",
      " [0.99999994]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [0.99999833]\n",
      " [1.        ]\n",
      " [0.99999833]]\n",
      "Loss: 0.999960720539\n",
      "Prediction:[[0.99976254]\n",
      " [0.99999994]\n",
      " [0.9999998 ]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [0.99999803]\n",
      " [1.        ]\n",
      " [0.99999803]]\n",
      "Loss: 0.999951660633\n",
      "Prediction:[[0.9996931 ]\n",
      " [0.99999994]\n",
      " [0.99999976]\n",
      " [0.99999994]\n",
      " [0.9999997 ]\n",
      " [1.        ]\n",
      " [0.99999976]\n",
      " [0.99999774]\n",
      " [1.        ]\n",
      " [0.9999977 ]]\n",
      "Loss: 0.999937534332\n",
      "Prediction:[[0.9995727 ]\n",
      " [0.99999994]\n",
      " [0.9999995 ]\n",
      " [1.        ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [0.9999997 ]\n",
      " [0.99999744]\n",
      " [1.        ]\n",
      " [0.99999696]]\n",
      "Loss: 0.999913215637\n",
      "Prediction:[[0.9993232 ]\n",
      " [0.9999998 ]\n",
      " [0.9999994 ]\n",
      " [0.9999999 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.99999934]\n",
      " [0.9999968 ]\n",
      " [1.        ]\n",
      " [0.99999577]]\n",
      "Loss: 0.999862849712\n",
      "Prediction:[[0.99859947]\n",
      " [1.        ]\n",
      " [0.9999988 ]\n",
      " [0.99999976]\n",
      " [0.99999976]\n",
      " [1.        ]\n",
      " [0.99999905]\n",
      " [0.99999547]\n",
      " [1.        ]\n",
      " [0.99999297]]\n",
      "Loss: 0.999717235565\n",
      "Prediction:[[0.99373096]\n",
      " [0.9999997 ]\n",
      " [0.99999684]\n",
      " [0.9999999 ]\n",
      " [0.9999993 ]\n",
      " [0.9999999 ]\n",
      " [0.99999744]\n",
      " [0.999991  ]\n",
      " [1.        ]\n",
      " [0.99997956]]\n",
      "Loss: 0.998742878437\n",
      "Prediction:[[-0.41844365]\n",
      " [ 0.99995065]\n",
      " [ 0.9995965 ]\n",
      " [ 0.9999874 ]\n",
      " [ 0.99992806]\n",
      " [ 0.9999905 ]\n",
      " [ 0.9997745 ]\n",
      " [ 0.9997836 ]\n",
      " [ 0.99999547]\n",
      " [ 0.9977449 ]]\n",
      "Loss: 0.91686040163\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n",
      "Prediction:[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: 1.0\n"
     ]
    }
   ],
   "source": [
    "encoder_after_training = train_restored_model_enc_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898806 ,  0.5807226 ,  0.49744806, ...,  0.5166379 ,\n",
       "        -0.03120399, -0.18058273],\n",
       "       [-0.4750856 ,  0.721219  ,  0.6492678 , ...,  0.3919891 ,\n",
       "        -0.02343164, -0.18083476],\n",
       "       [-0.3480006 ,  0.43519476,  0.2753141 , ...,  0.19185491,\n",
       "        -0.29673263, -0.18144843],\n",
       "       ...,\n",
       "       [ 0.03478556,  0.4519019 ,  0.0941446 , ...,  0.3134694 ,\n",
       "        -0.06109187,  0.03018548],\n",
       "       [-0.18481615,  0.31060803,  0.45174283, ...,  0.47246057,\n",
       "        -0.11575688, -0.19709298],\n",
       "       [-0.7678361 ,  0.7076941 ,  0.39733076, ...,  0.21106088,\n",
       "        -0.06787533, -0.23257294]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'word_embeds:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'linear/Variable:0' shape=(200, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'linear/Variable_1:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'word_embeds_1:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'linear_1/Variable:0' shape=(200, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'linear_1/Variable_1:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_collection('variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898806 ,  0.5807226 ,  0.49744806, ...,  0.5166379 ,\n",
       "        -0.03120399, -0.18058273],\n",
       "       [-0.4750856 ,  0.721219  ,  0.6492678 , ...,  0.3919891 ,\n",
       "        -0.02343164, -0.18083476],\n",
       "       [-0.3480006 ,  0.43519476,  0.2753141 , ...,  0.19185491,\n",
       "        -0.29673263, -0.18144843],\n",
       "       ...,\n",
       "       [ 0.03478556,  0.4519019 ,  0.0941446 , ...,  0.3134694 ,\n",
       "        -0.06109187,  0.03018548],\n",
       "       [-0.18481615,  0.31060803,  0.45174283, ...,  0.47246057,\n",
       "        -0.11575688, -0.19709298],\n",
       "       [-0.7678361 ,  0.7076941 ,  0.39733076, ...,  0.21106088,\n",
       "        -0.06787533, -0.23257294]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_meta_path = '../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_saver = tf.train.import_meta_graph(model_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc_graph = tf.get_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'word_embeds:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'linear/Variable:0' shape=(200, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'linear/Variable_1:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'word_embeds:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'word_embeds_1:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'word_embeds_2:0' shape=(7056, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/kernel:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable:0' shape=(100, 7056) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1:0' shape=(7056,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/kernel:0' shape=(300, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/bias:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adam:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/kernel/Adam_1:0' shape=(250, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/bias/Adam:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'seq2seq_encoder/bidirectional_rnn/fw/lstm_cell/bias/Adam_1:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam:0' shape=(100, 7056) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable/Adam_1:0' shape=(100, 7056) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1/Adam:0' shape=(7056,) dtype=float32_ref>,\n",
       " <tf.Variable 'Variable_1/Adam_1:0' shape=(7056,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/kernel/Adam:0' shape=(300, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/kernel/Adam_1:0' shape=(300, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/bias/Adam:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/lstm_cell/bias/Adam_1:0' shape=(400,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_graph.get_collection('variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'seq2seq_encoder:0' refers to a Tensor which does not exist. The operation, 'seq2seq_encoder', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-98c33c977017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seq2seq_encoder:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3205\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[1;32m   3206\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3075\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   3076\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3077\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'seq2seq_encoder:0' refers to a Tensor which does not exist. The operation, 'seq2seq_encoder', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "enc = enc_graph.get_tensor_by_name(\"seq2seq_encoder:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized session\n",
      "loading existing model\n",
      "INFO:tensorflow:Restoring parameters from ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key linear/Variable not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op u'save/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-bf733d7af729>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key linear/Variable not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-64a0eca89962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_useful_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestored_model_enc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-522c2a6286aa>\u001b[0m in \u001b[0;36mrestored_model_enc_out\u001b[0;34m(model_exists_already)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_exists_already\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading existing model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key linear/Variable not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op u'save/RestoreV2', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-bf733d7af729>\", line 3, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key linear/Variable not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "encoder_useful_state = restored_model_enc_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898806 ,  0.5807226 ,  0.49744806, ...,  0.5166379 ,\n",
       "        -0.03120399, -0.18058273],\n",
       "       [-0.4750856 ,  0.721219  ,  0.6492678 , ...,  0.3919891 ,\n",
       "        -0.02343164, -0.18083476],\n",
       "       [-0.3480006 ,  0.43519476,  0.2753141 , ...,  0.19185491,\n",
       "        -0.29673263, -0.18144843],\n",
       "       ...,\n",
       "       [ 0.03478556,  0.4519019 ,  0.0941446 , ...,  0.3134694 ,\n",
       "        -0.06109187,  0.03018548],\n",
       "       [-0.18481615,  0.31060803,  0.45174283, ...,  0.47246057,\n",
       "        -0.11575688, -0.19709298],\n",
       "       [-0.7678361 ,  0.7076941 ,  0.39733076, ...,  0.21106088,\n",
       "        -0.06787533, -0.23257294]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bridge(self):\n",
    "    '''This creates a bridge to our seq2seq model\n",
    "    The bridge simulates removal of words for each given sentence\n",
    "    self.word_ids\n",
    "    self.sequence_lengths\n",
    "    max_seq_length\n",
    "    '''\n",
    "    with tf.variable_scope(\"drop_words\"):\n",
    "        '''Create mask based on max seq length'''\n",
    "        max_seq_length = word_ids.shape[-1] \n",
    "        np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "        a = np.array(range(max_seq_length))\n",
    "        np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "        tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "        padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "        \n",
    "        #2nd Operation Add a dimension to both input tensors\n",
    "        resultant_tensor_for_enc = tf.expand_dims(word_ids,0)\n",
    "        tensor_seq_lengths_for_enc = tf.expand_dims(sequence_lengths, 0)\n",
    "        #3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "        seq_lengths_for_dropped = tf.expand_dims(sequence_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "\n",
    "        #4th operation looped-> Applying mask matrix and related operations to get\n",
    "        #resultant tensor and tensor seq lens \n",
    "        #THESE ARE APPENDED to (don't know if that's good)\n",
    "        for drop_index in range(max_seq_length):\n",
    "            '''Create mask function-> note the drop index changes at each iteration'''\n",
    "            f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "            '''Apply masking and padding'''\n",
    "            resultant_tensor_for_enc = tf.concat([resultant_tensor_for_enc,tf.expand_dims(tf.pad(tf.map_fn(f, word_ids), padding, \"CONSTANT\"),0)],0)\n",
    "            tensor_seq_lengths_for_enc = tf.concat([tensor_seq_lengths_for_enc, seq_lengths_for_dropped],0)\n",
    "\n",
    "        #5th operation--> reshape of tensor\n",
    "        resultant_tensor_for_enc = tf.reshape(resultant_tensor_for_enc,[resultant_tensor_for_enc.shape[0]*resultant_tensor_for_enc.shape[1],resultant_tensor_for_enc.shape[2]])\n",
    "        tensor_seq_lengths_for_enc = tf.reshape(tensor_seq_lengths_for_enc,[tensor_seq_lengths_for_enc.shape[0]*tensor_seq_lengths_for_enc.shape[1],])\n",
    "\n",
    "        \n",
    "def seq2seq_enc(self):\n",
    "    #@TODO\n",
    "    '''Replace encoder_inputs and encoder_inputs_length with general self.word_ids and self.sequence_lengths'''\n",
    "    '''This is the pretrained encoder'''\n",
    "    with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, resultant_tensor_for_enc)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=resultant_tensor_for_enc,\n",
    "                                    sequence_length= tensor_seq_lengths_for_enc,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6958944e1ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m'''Create mask based on max seq length'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mnp_mask_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#num drops=n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_ids' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"drop_words\"):\n",
    "        '''Create mask based on max seq length'''\n",
    "        max_seq_length = word_ids.shape[-1] \n",
    "        np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "        a = np.array(range(max_seq_length))\n",
    "        np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "        tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "        padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "        \n",
    "        #2nd Operation Add a dimension to both input tensors\n",
    "        resultant_tensor_for_enc = tf.expand_dims(word_ids,0)\n",
    "        tensor_seq_lengths_for_enc = tf.expand_dims(sequence_lengths, 0)\n",
    "        #3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "        seq_lengths_for_dropped = tf.expand_dims(sequence_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "\n",
    "        #4th operation looped-> Applying mask matrix and related operations to get\n",
    "        #resultant tensor and tensor seq lens \n",
    "        #THESE ARE APPENDED to (don't know if that's good)\n",
    "        for drop_index in range(max_seq_length):\n",
    "            '''Create mask function-> note the drop index changes at each iteration'''\n",
    "            f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "            '''Apply masking and padding'''\n",
    "            resultant_tensor_for_enc = tf.concat([resultant_tensor_for_enc,tf.expand_dims(tf.pad(tf.map_fn(f, word_ids), padding, \"CONSTANT\"),0)],0)\n",
    "            tensor_seq_lengths_for_enc = tf.concat([tensor_seq_lengths_for_enc, seq_lengths_for_dropped],0)\n",
    "\n",
    "        #5th operation--> reshape of tensor\n",
    "        resultant_tensor_for_enc = tf.reshape(resultant_tensor_for_enc,[resultant_tensor_for_enc.shape[0]*resultant_tensor_for_enc.shape[1],resultant_tensor_for_enc.shape[2]])\n",
    "        tensor_seq_lengths_for_enc = tf.reshape(tensor_seq_lengths_for_enc,[tensor_seq_lengths_for_enc.shape[0]*tensor_seq_lengths_for_enc.shape[1],])\n",
    "\n",
    "        \n",
    "with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, resultant_tensor_for_enc)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=resultant_tensor_for_enc,\n",
    "                                    sequence_length= tensor_seq_lengths_for_enc,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_encoder_reps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-68e1500b5742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m'''get_encoder_reps returns the batched encoder representations (shape = [(input_max_length+2)*(input_batch_size) , encoder_dims]'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#x = get_encoder_reps(resultant_tensor,tensor_seq_lengths,dims)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mseq2seq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoder_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor_seq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_word_seq_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#Each column corresponds to the vector rep for a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_encoder_reps' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'''INPUTS'''\n",
    "input_word_seq_tensor = tf.convert_to_tensor(np.array([[0,1,2,3,1,2],[9,10,11,12,13,12]]),dtype='int32')\n",
    "max_seq_length = 6\n",
    "enc_dims = 50\n",
    "seq_lengths = tf.convert_to_tensor(np.array([4,5]),dtype='int32') #shape n\n",
    "\n",
    "\n",
    "\n",
    "#1st OPERATION-> Create mask based on max seq length\n",
    "'''Create mask based on max seq length'''\n",
    "np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "a = np.array(range(max_seq_length))\n",
    "np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "\n",
    "#2nd Operation Add a dimension to both input tensors\n",
    "resultant_tensor = tf.expand_dims(input_word_seq_tensor,0)\n",
    "tensor_seq_lengths = tf.expand_dims(seq_lengths, 0)\n",
    "#3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "seq_lengths_for_dropped = tf.expand_dims(seq_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "#print(seq_lengths_for_dropped)\n",
    "\n",
    "#def condition(x):\n",
    " #   x.shape[0]< max_seq_length\n",
    "#def body(x):\n",
    " #   Copy\n",
    "\n",
    "#4th operation looped-> Applying mask matrix and related operations to get\n",
    "#resultant tensor and tensor seq lens \n",
    "#THESE ARE APPENDED to (don't know if that's good)\n",
    "for drop_index in range(max_seq_length):\n",
    "    '''Create mask function-> note the drop index changes at each iteration'''\n",
    "    f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "    '''Apply masking and padding'''\n",
    "    #temp = tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\") #We apply mask and then pad result to enable concatenation with original \n",
    "    #temp = tf.expand_dims(temp,0)\n",
    "    resultant_tensor = tf.concat([resultant_tensor,tf.expand_dims(tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\"),0)],0)\n",
    "    tensor_seq_lengths = tf.concat([tensor_seq_lengths, seq_lengths_for_dropped],0)\n",
    "\n",
    "#5th operation--> reshape of tensor\n",
    "resultant_tensor = tf.reshape(resultant_tensor,[resultant_tensor.shape[0]*resultant_tensor.shape[1],resultant_tensor.shape[2]])\n",
    "tensor_seq_lengths = tf.reshape(tensor_seq_lengths,[tensor_seq_lengths.shape[0]*tensor_seq_lengths.shape[1],])\n",
    "\n",
    "#6th operation --> feed to seq2seq encoder and reshape output\n",
    "'''get_encoder_reps returns the batched encoder representations (shape = [(input_max_length+2)*(input_batch_size) , encoder_dims]'''\n",
    "#x = get_encoder_reps(resultant_tensor,tensor_seq_lengths,dims)\n",
    "seq2seq_out = tf.reshape(get_encoder_reps(resultant_tensor,tensor_seq_lengths), [max_seq_length+1, input_word_seq_tensor.shape[0], enc_dims])\n",
    "\n",
    "#Each column corresponds to the vector rep for a sentence\n",
    "#Each row corresponds to the word representation\n",
    "\n",
    "#7th operation --> Obtain the subtracted values/opd values for reps\n",
    "#We now reduce this by doing an op(row_0,row_i) for all columns\n",
    "'''NOTE : Hard coded here with subtract have to replace with op(row_0, row_i) for all rows i'''\n",
    "seq2seq_out = tf.subtract(seq2seq_out,seq2seq_out[0,:])[1:,:]\n",
    "#So final output should be max_seq_len*n_batches*dims --> do transpose if needed\n",
    "#8th operation\n",
    "seq2seq_out = tf.transpose(seq2seq_out,perm=[1,0,2]) #convert to n_batches*max_seq_len*dims or n_batches*words*dims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply function operation that takes first index and performs op(first_index, slice)\n",
    "#print(seq2seq_in.eval())\n",
    "#print(seq2seq_in.shape)\n",
    "#print(seq2seq_in_lengths.eval())\n",
    "#print(seq2seq_in_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. If we put seq2seq code inside the other code with all variables\n",
    "2. Train just the seq2seq (cost, decoded outs, etc)\n",
    "3. Then save this model with all other variables intact\n",
    "4. When model is restored we are restoring both seq2seq weights as well as the others and the whole complication of \n",
    "5. ONLY complication-> How to set other variables as not trainable \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
