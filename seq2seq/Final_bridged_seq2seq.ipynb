{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-3887a96143dd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-3887a96143dd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Q1: Should we use a loop with concatenation or a predefined zeros tensor for creation of dropped indices\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q1: Should we use a loop with concatenation or a predefined zeros tensor for creation of dropped indices\n",
    "    \n",
    "\n",
    "# What we want to do:\n",
    "#1) Load two models together--> Loading happens through a session which restores variables, etc. \n",
    "#2) We wish to just load the encoder variables (pretrained) and add them to our computation for composite model\n",
    " \n",
    "\n",
    "#What we try doing is based on the following logic:\n",
    "#1) A graph defines all operations and data flow. \n",
    "#2) However, given that a graph can be invoked through a session, we will need to first create a session. \n",
    "#3) QUes: Can a graph only be invoked through a session\n",
    "\n",
    "#4) Then we load this graph and import the encoder part of it into our composite graph.\n",
    "#5) We finally run a session for this composite graph. \n",
    "\n",
    "\n",
    "\n",
    "#6) HOWEVER, we need to make sure that the encoder is not trained any further. \n",
    "#7) Otherwise, we will continue with just two separate sessions. \n",
    "\n",
    "\n",
    "'''\n",
    "The final procedure:\n",
    "1) Create session to load pretrained graph. Store the graph. Quite session. We do this only once. A graph has to be declared in advance. \n",
    "2) Set the encoder weights to be non trainable. \n",
    "3) Create new session to train original model and use encoder as a part of the operation flow. \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#For optimization and graph construction:\n",
    "https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html\n",
    "    \n",
    "#General overview of graphs\n",
    "https://www.tensorflow.org/programmers_guide/graphs#visualizing_your_graph\n",
    "    \n",
    "#Making non trainable variables\n",
    "https://stackoverflow.com/questions/37326002/is-it-possible-to-make-a-trainable-variable-not-trainable?rq=1\n",
    "https://stackoverflow.com/questions/35298326/freeze-some-variables-scopes-in-tensorflow-stop-gradient-vs-passing-variables\n",
    "    \n",
    "#Loading multiple graphs:\n",
    "https://stackoverflow.com/questions/41990014/load-multiple-models-in-tensorflow\n",
    "    \n",
    "#Loading trained weights from one graph to another\n",
    "https://stackoverflow.com/questions/39068703/tensorflow-using-weights-trained-in-one-model-inside-another-different-model?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../model/')\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(filename):\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "        \n",
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domain_name = 'laptop'\n",
    "domain_tr_data_path = '../data/Final_joint_data_absa//Domains/Laptop/Normal__normal_training_list.pickle'\n",
    "embeddings_path = '../data/Embeddings/Pruned/np_glove_200d_trimmed.npz'\n",
    "embeddings_name = 'glove200d'\n",
    "vocab_path = '../data/vocab_to_id.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "word_embeddings_np = get_word_embeddings(embeddings_path)\n",
    "pad_token = '<PAD>' \n",
    "eos_token = '<END>'\n",
    "PAD = vocab[pad_token]\n",
    "EOS = vocab[eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 200\n",
    "encoder_hidden_units = 50 #100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_ids = tf.placeholder(shape=(None, None), dtype=tf.int32, name='word_ids')\n",
    "sequence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='sequence_lengths')\n",
    "embeddings = tf.Variable(word_embeddings_np, name=\"word_embeds\",dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''This is just to test again'''\n",
    "\n",
    "\n",
    "with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, word_ids)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length= sequence_lengths,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n",
    "\n",
    "with tf.variable_scope(\"linear\"):\n",
    "    v1 = tf.Variable(tf.random_normal[encoder_concat_rep.shape[-1],10])\n",
    "    b1 = tf.Variable(tf.random_normal[encoder_concat_re])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data\n",
    "\n",
    "def get_holdout_data(idd_domain_tr_data, num=10):\n",
    "    '''Simple function to check if encoder is functioning properly'''\n",
    "    return idd_domain_tr_data[:num]\n",
    "\n",
    "def batch_modify(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = PAD*np.ones(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths\n",
    "\n",
    "def feed_enc(enc_batch):\n",
    "    \n",
    "    encoder_inputs_, encoder_input_lengths_ = batch_modify(enc_batch)\n",
    "    return {word_ids: encoder_inputs_,\n",
    "            sequence_lengths: encoder_input_lengths_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idd_data = dataset_load(domain_tr_data_path,vocab_path)\n",
    "holdout_data = get_holdout_data(idd_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embed_type = \"Glove\"\n",
    "model_path = '../results/seq2seq/{}_seq2seqmodel_embeds{}_{}d_{}hiddenunits.ckpt'.format(domain_name,embed_type,input_embedding_size,encoder_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Cell to test in format for experiment'''\n",
    "def restored_model_enc_out(model_exists_already=True):\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(\"Initialized session\")\n",
    "        if(model_exists_already):\n",
    "            print(\"loading existing model\")\n",
    "            saver.restore(sess, model_path)\n",
    "        \n",
    "       \n",
    "        f_enc = feed_enc(holdout_data)\n",
    "\n",
    "        encoder_useful_state = sess.run(encoder_concat_rep, f_enc)\n",
    "            \n",
    "        #encoder_useful_state = sess.run(encoder_concat_everything)\n",
    "    return encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized session\n",
      "loading existing model\n",
      "INFO:tensorflow:Restoring parameters from ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    }
   ],
   "source": [
    "encoder_useful_state = restored_model_enc_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6898806 ,  0.5807226 ,  0.49744806, ...,  0.5166379 ,\n",
       "        -0.03120399, -0.18058273],\n",
       "       [-0.4750856 ,  0.721219  ,  0.6492678 , ...,  0.3919891 ,\n",
       "        -0.02343164, -0.18083476],\n",
       "       [-0.3480006 ,  0.43519476,  0.2753141 , ...,  0.19185491,\n",
       "        -0.29673263, -0.18144843],\n",
       "       ...,\n",
       "       [ 0.03478556,  0.4519019 ,  0.0941446 , ...,  0.3134694 ,\n",
       "        -0.06109187,  0.03018548],\n",
       "       [-0.18481615,  0.31060803,  0.45174283, ...,  0.47246057,\n",
       "        -0.11575688, -0.19709298],\n",
       "       [-0.7678361 ,  0.7076941 ,  0.39733076, ...,  0.21106088,\n",
       "        -0.06787533, -0.23257294]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bridge(self):\n",
    "    '''This creates a bridge to our seq2seq model\n",
    "    The bridge simulates removal of words for each given sentence\n",
    "    self.word_ids\n",
    "    self.sequence_lengths\n",
    "    max_seq_length\n",
    "    '''\n",
    "    with tf.variable_scope(\"drop_words\"):\n",
    "        '''Create mask based on max seq length'''\n",
    "        max_seq_length = word_ids.shape[-1] \n",
    "        np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "        a = np.array(range(max_seq_length))\n",
    "        np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "        tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "        padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "        \n",
    "        #2nd Operation Add a dimension to both input tensors\n",
    "        resultant_tensor_for_enc = tf.expand_dims(word_ids,0)\n",
    "        tensor_seq_lengths_for_enc = tf.expand_dims(sequence_lengths, 0)\n",
    "        #3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "        seq_lengths_for_dropped = tf.expand_dims(sequence_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "\n",
    "        #4th operation looped-> Applying mask matrix and related operations to get\n",
    "        #resultant tensor and tensor seq lens \n",
    "        #THESE ARE APPENDED to (don't know if that's good)\n",
    "        for drop_index in range(max_seq_length):\n",
    "            '''Create mask function-> note the drop index changes at each iteration'''\n",
    "            f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "            '''Apply masking and padding'''\n",
    "            resultant_tensor_for_enc = tf.concat([resultant_tensor_for_enc,tf.expand_dims(tf.pad(tf.map_fn(f, word_ids), padding, \"CONSTANT\"),0)],0)\n",
    "            tensor_seq_lengths_for_enc = tf.concat([tensor_seq_lengths_for_enc, seq_lengths_for_dropped],0)\n",
    "\n",
    "        #5th operation--> reshape of tensor\n",
    "        resultant_tensor_for_enc = tf.reshape(resultant_tensor_for_enc,[resultant_tensor_for_enc.shape[0]*resultant_tensor_for_enc.shape[1],resultant_tensor_for_enc.shape[2]])\n",
    "        tensor_seq_lengths_for_enc = tf.reshape(tensor_seq_lengths_for_enc,[tensor_seq_lengths_for_enc.shape[0]*tensor_seq_lengths_for_enc.shape[1],])\n",
    "\n",
    "        \n",
    "def seq2seq_enc(self):\n",
    "    #@TODO\n",
    "    '''Replace encoder_inputs and encoder_inputs_length with general self.word_ids and self.sequence_lengths'''\n",
    "    '''This is the pretrained encoder'''\n",
    "    with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, resultant_tensor_for_enc)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=resultant_tensor_for_enc,\n",
    "                                    sequence_length= tensor_seq_lengths_for_enc,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6958944e1ea9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m'''Create mask based on max seq length'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mnp_mask_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#num drops=n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_ids' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"drop_words\"):\n",
    "        '''Create mask based on max seq length'''\n",
    "        max_seq_length = word_ids.shape[-1] \n",
    "        np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "        a = np.array(range(max_seq_length))\n",
    "        np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "        tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "        padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "        \n",
    "        #2nd Operation Add a dimension to both input tensors\n",
    "        resultant_tensor_for_enc = tf.expand_dims(word_ids,0)\n",
    "        tensor_seq_lengths_for_enc = tf.expand_dims(sequence_lengths, 0)\n",
    "        #3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "        seq_lengths_for_dropped = tf.expand_dims(sequence_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "\n",
    "        #4th operation looped-> Applying mask matrix and related operations to get\n",
    "        #resultant tensor and tensor seq lens \n",
    "        #THESE ARE APPENDED to (don't know if that's good)\n",
    "        for drop_index in range(max_seq_length):\n",
    "            '''Create mask function-> note the drop index changes at each iteration'''\n",
    "            f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "            '''Apply masking and padding'''\n",
    "            resultant_tensor_for_enc = tf.concat([resultant_tensor_for_enc,tf.expand_dims(tf.pad(tf.map_fn(f, word_ids), padding, \"CONSTANT\"),0)],0)\n",
    "            tensor_seq_lengths_for_enc = tf.concat([tensor_seq_lengths_for_enc, seq_lengths_for_dropped],0)\n",
    "\n",
    "        #5th operation--> reshape of tensor\n",
    "        resultant_tensor_for_enc = tf.reshape(resultant_tensor_for_enc,[resultant_tensor_for_enc.shape[0]*resultant_tensor_for_enc.shape[1],resultant_tensor_for_enc.shape[2]])\n",
    "        tensor_seq_lengths_for_enc = tf.reshape(tensor_seq_lengths_for_enc,[tensor_seq_lengths_for_enc.shape[0]*tensor_seq_lengths_for_enc.shape[1],])\n",
    "\n",
    "        \n",
    "with tf.variable_scope('seq2seq_encoder'):\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, resultant_tensor_for_enc)\n",
    "        encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "        ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=resultant_tensor_for_enc,\n",
    "                                    sequence_length= tensor_seq_lengths_for_enc,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) \n",
    "    \n",
    "        encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "        encoder_final_state_c = tf.concat(\n",
    "            (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "        encoder_final_state_h = tf.concat(\n",
    "            (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "        encoder_final_state = LSTMStateTuple(\n",
    "            c=encoder_final_state_c,\n",
    "            h=encoder_final_state_h\n",
    "            ) #this is useful later\n",
    "\n",
    "        encoder_concat_rep = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_encoder_reps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-68e1500b5742>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m'''get_encoder_reps returns the batched encoder representations (shape = [(input_max_length+2)*(input_batch_size) , encoder_dims]'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#x = get_encoder_reps(resultant_tensor,tensor_seq_lengths,dims)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mseq2seq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_encoder_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor_seq_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_word_seq_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_dims\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#Each column corresponds to the vector rep for a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_encoder_reps' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "'''INPUTS'''\n",
    "input_word_seq_tensor = tf.convert_to_tensor(np.array([[0,1,2,3,1,2],[9,10,11,12,13,12]]),dtype='int32')\n",
    "max_seq_length = 6\n",
    "enc_dims = 50\n",
    "seq_lengths = tf.convert_to_tensor(np.array([4,5]),dtype='int32') #shape n\n",
    "\n",
    "\n",
    "\n",
    "#1st OPERATION-> Create mask based on max seq length\n",
    "'''Create mask based on max seq length'''\n",
    "np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "a = np.array(range(max_seq_length))\n",
    "np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "\n",
    "#2nd Operation Add a dimension to both input tensors\n",
    "resultant_tensor = tf.expand_dims(input_word_seq_tensor,0)\n",
    "tensor_seq_lengths = tf.expand_dims(seq_lengths, 0)\n",
    "#3rd operation -> Make tensor for seq_lengths for dropped indices (they're always 1 less)\n",
    "seq_lengths_for_dropped = tf.expand_dims(seq_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "#print(seq_lengths_for_dropped)\n",
    "\n",
    "#def condition(x):\n",
    " #   x.shape[0]< max_seq_length\n",
    "#def body(x):\n",
    " #   Copy\n",
    "\n",
    "#4th operation looped-> Applying mask matrix and related operations to get\n",
    "#resultant tensor and tensor seq lens \n",
    "#THESE ARE APPENDED to (don't know if that's good)\n",
    "for drop_index in range(max_seq_length):\n",
    "    '''Create mask function-> note the drop index changes at each iteration'''\n",
    "    f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "    '''Apply masking and padding'''\n",
    "    #temp = tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\") #We apply mask and then pad result to enable concatenation with original \n",
    "    #temp = tf.expand_dims(temp,0)\n",
    "    resultant_tensor = tf.concat([resultant_tensor,tf.expand_dims(tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\"),0)],0)\n",
    "    tensor_seq_lengths = tf.concat([tensor_seq_lengths, seq_lengths_for_dropped],0)\n",
    "\n",
    "#5th operation--> reshape of tensor\n",
    "resultant_tensor = tf.reshape(resultant_tensor,[resultant_tensor.shape[0]*resultant_tensor.shape[1],resultant_tensor.shape[2]])\n",
    "tensor_seq_lengths = tf.reshape(tensor_seq_lengths,[tensor_seq_lengths.shape[0]*tensor_seq_lengths.shape[1],])\n",
    "\n",
    "#6th operation --> feed to seq2seq encoder and reshape output\n",
    "'''get_encoder_reps returns the batched encoder representations (shape = [(input_max_length+2)*(input_batch_size) , encoder_dims]'''\n",
    "#x = get_encoder_reps(resultant_tensor,tensor_seq_lengths,dims)\n",
    "seq2seq_out = tf.reshape(get_encoder_reps(resultant_tensor,tensor_seq_lengths), [max_seq_length+1, input_word_seq_tensor.shape[0], enc_dims])\n",
    "\n",
    "#Each column corresponds to the vector rep for a sentence\n",
    "#Each row corresponds to the word representation\n",
    "\n",
    "#7th operation --> Obtain the subtracted values/opd values for reps\n",
    "#We now reduce this by doing an op(row_0,row_i) for all columns\n",
    "'''NOTE : Hard coded here with subtract have to replace with op(row_0, row_i) for all rows i'''\n",
    "seq2seq_out = tf.subtract(seq2seq_out,seq2seq_out[0,:])[1:,:]\n",
    "#So final output should be max_seq_len*n_batches*dims --> do transpose if needed\n",
    "#8th operation\n",
    "seq2seq_out = tf.transpose(seq2seq_out,perm=[1,0,2]) #convert to n_batches*max_seq_len*dims or n_batches*words*dims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply function operation that takes first index and performs op(first_index, slice)\n",
    "#print(seq2seq_in.eval())\n",
    "#print(seq2seq_in.shape)\n",
    "#print(seq2seq_in_lengths.eval())\n",
    "#print(seq2seq_in_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
