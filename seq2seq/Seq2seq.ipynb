{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TF Seq2seq\n",
    "1) For seq2seq to work we need eos and pad tokens. We use pad as <UNK> since there are no major repurcussions, and eos is stored in vocab already. Unless, <UNK> has a particular embedding. \n",
    "2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../model/')\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_word_embeddings(filename):\n",
    "    try:\n",
    "        with np.load(filename) as data:\n",
    "            return data[\"embeddings\"]\n",
    "\n",
    "    except IOError:\n",
    "        raise MyIOError(filename)\n",
    "        \n",
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "domain_name = 'laptop'\n",
    "domain_tr_data_path = '../data/Final_joint_data_absa//Domains/Laptop/Normal__normal_training_list.pickle'\n",
    "embeddings_path = '../data/Embeddings/Pruned/np_glove_200d_trimmed.npz'\n",
    "embeddings_name = 'glove200d'\n",
    "vocab_path = '../data/vocab_to_id.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "word_embeddings_np = get_word_embeddings(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pad_token = '<PAD>' \n",
    "eos_token = '<END>'\n",
    "PAD = vocab[pad_token]\n",
    "EOS = vocab[eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "input_embedding_size = 200\n",
    "encoder_hidden_units = 50 #100\n",
    "decoder_hidden_units = encoder_hidden_units*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(word_embeddings_np, name=\"word_embeds\",dtype=tf.float32, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) #'\n",
    "    \n",
    "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "encoder_final_state_c = tf.concat(\n",
    "        (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "encoder_final_state_h = tf.concat(\n",
    "        (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "encoder_final_state = LSTMStateTuple(\n",
    "        c=encoder_final_state_c,\n",
    "        h=encoder_final_state_h\n",
    "    ) #this is useful later\n",
    "\n",
    "encoder_concat_everything = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-fc4769ca3ba3>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-fc4769ca3ba3>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    '''THIS STUFF NEEDS TO BE EDITED, just copy the encoder part into original code'''\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "with tf.variable_scope(\"seq2seq_encoder\"):\n",
    "    '''THIS STUFF NEEDS TO BE EDITED, just copy the encoder part into original code'''\n",
    "    encoder_cell = LSTMCell(encoder_hidden_units)\n",
    "    ((encoder_fw_outputs,\n",
    "          encoder_bw_outputs),\n",
    "         (encoder_fw_final_state,\n",
    "          encoder_bw_final_state)) = (\n",
    "            tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
    "                                    cell_bw=encoder_cell,\n",
    "                                    inputs=encoder_inputs_embedded,\n",
    "                                    sequence_length=encoder_inputs_length,\n",
    "                                    dtype=tf.float32, time_major=True)\n",
    "            ) #'\n",
    "    \n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs),2)\n",
    "    \n",
    "\n",
    "    encoder_final_state_c = tf.concat(\n",
    "        (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\n",
    "    encoder_final_state_h = tf.concat(\n",
    "        (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\n",
    "    encoder_final_state = LSTMStateTuple(\n",
    "        c=encoder_final_state_c,\n",
    "        h=encoder_final_state_h\n",
    "    ) #this is useful later\n",
    "\n",
    "    encoder_concat_everything = tf.concat([encoder_final_state_c,encoder_final_state_h], 1)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_cell = LSTMCell(decoder_hidden_units)\n",
    "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_lengths = encoder_inputs_length + 4 #3 additional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eos_time_slice = EOS*tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
    "pad_time_slice = PAD*tf.ones([batch_size], dtype=tf.int32, name='PAD')\n",
    "\n",
    "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
    "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loop_fn_initial():\n",
    "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
    "    initial_input = eos_step_embedded\n",
    "    initial_cell_state = encoder_final_state\n",
    "    initial_cell_output = None\n",
    "    initial_loop_state = None  # we don't need to pass any additional information\n",
    "    return (initial_elements_finished,\n",
    "            initial_input,\n",
    "            initial_cell_state,\n",
    "            initial_cell_output,\n",
    "            initial_loop_state)\n",
    "\n",
    "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
    "\n",
    "    def get_next_input():\n",
    "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
    "        prediction = tf.argmax(output_logits, axis=1)\n",
    "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
    "        return next_input\n",
    "    \n",
    "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
    "                                                  # defining if corresponding sequence has ended\n",
    "\n",
    "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
    "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
    "    state = previous_state\n",
    "    output = previous_output\n",
    "    loop_state = None\n",
    "\n",
    "    return (elements_finished, \n",
    "            input,\n",
    "            state,\n",
    "            output,\n",
    "            loop_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
    "    if previous_state is None:    # time == 0\n",
    "        assert previous_output is None and previous_state is None\n",
    "        return loop_fn_initial()\n",
    "    else:\n",
    "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
    "\n",
    "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
    "decoder_outputs = decoder_outputs_ta.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
    "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
    "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
    "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "num_epochs = 10\n",
    "\n",
    "def dataset_load(domain_tr_data_path, vocab_path):\n",
    "    with open(domain_tr_data_path,'r') as p1:\n",
    "        domain_tr_data = pickle.load(p1)\n",
    "    with open(vocab_path,'r') as p1:\n",
    "        vocab = pickle.load(p1)\n",
    "        \n",
    "    domain_tr_data = map(lambda x: x[0],domain_tr_data)\n",
    "    idd_domain_tr_data = map(lambda x: [vocab[word] for word in x], domain_tr_data)\n",
    "    return idd_domain_tr_data\n",
    "\n",
    "def get_holdout_data(idd_domain_tr_data, num=10):\n",
    "    '''Simple function to check if encoder is functioning properly'''\n",
    "    return idd_domain_tr_data[:num]\n",
    "\n",
    "def gen_batch(idd_data, batch_size):\n",
    "    np.random.shuffle(idd_data)\n",
    "    rem = len(idd_data)%batch_size\n",
    "    num_batches = (len(idd_data)/batch_size) \n",
    "    if rem>0:\n",
    "        num_batches = num_batches + 1\n",
    "    \n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        if(i == num_batches -1 and (not rem==0)):\n",
    "            yield(idd_data[i*batch_size:])\n",
    "        else:\n",
    "            yield(idd_data[i*batch_size:(i+1)*batch_size])\n",
    "            \n",
    "def batch_modify(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = PAD*np.ones(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths\n",
    "        \n",
    "def next_feed(batch):\n",
    "    encoder_inputs_, encoder_input_lengths_ = batch_modify(batch)\n",
    "    decoder_targets_, _ = batch_modify(\n",
    "        [(sequence) + [EOS] + [PAD] * 3 for sequence in batch] #additional 3 spaces\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        encoder_inputs_length: encoder_input_lengths_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }\n",
    "\n",
    "def feed_enc(enc_batch):\n",
    "    \n",
    "    encoder_inputs_, encoder_input_lengths_ = batch_modify(enc_batch)\n",
    "    return {encoder_inputs: encoder_inputs_,\n",
    "            encoder_inputs_length: encoder_input_lengths_}\n",
    "\n",
    "\n",
    "def feed_enc_with_bridge(encoder_inputs, encoder_input_lengths_):\n",
    "    \n",
    "    return {encoder_inputs: encoder_inputs_,\n",
    "            encoder_inputs_length: encoder_input_lengths_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idd_data = dataset_load(domain_tr_data_path,vocab_path)\n",
    "holdout_data = get_holdout_data(idd_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "embed_type = \"Glove\"\n",
    "model_path = '../results/seq2seq/{}_seq2seqmodel_embeds{}_{}d_{}hiddenunits.ckpt'.format(domain_name,embed_type,input_embedding_size,encoder_hidden_units)\n",
    "encoder_path = '../results/seq2seq/{}_ENC_seq2seqmodel_embeds{}_{}d_{}hiddenunits.ckpt'.format(domain_name,embed_type,input_embedding_size,encoder_hidden_units)\n",
    "batch_size = 150\n",
    "lr = None\n",
    "num_epochs = 1\n",
    "model_exists_already = False\n",
    "\n",
    "if (lr is None):\n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "else:\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "saver = tf.train.Saver()\n",
    "#encoder_saver = tf.train.Saver({\"seq2seq_encoder\": encoder_concat_everything})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_track = []\n",
    "\n",
    "def train_graph(model_exists_already):\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(\"Initialized session\")\n",
    "        if(model_exists_already):\n",
    "            print(\"loading existing model\")\n",
    "            saver.restore(sess, model_path)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"At epoch: {}\".format(epoch))\n",
    "            iters = 0 \n",
    "            epoch_loss = 0.\n",
    "            batch_generator = gen_batch(idd_data, batch_size)\n",
    "            for batch in batch_generator:\n",
    "                \n",
    "                fd = next_feed(batch)\n",
    "                _, l = sess.run([train_op, loss], fd)\n",
    "    \n",
    "                if(iters%33==0):\n",
    "                    print(\"At batch: {}\".format(iters))\n",
    "                    print(\"Batch loss: {}\".format(l))\n",
    "                loss_track.append(l)\n",
    "                iters+=1\n",
    "                epoch_loss+=l\n",
    "            print(\"Epoch training loss: {}\".format(epoch_loss/iters))\n",
    "            \n",
    "        f_enc = feed_enc(holdout_data)\n",
    "        encoder_useful_state = sess.run(encoder_concat_everything, f_enc)\n",
    "            \n",
    "        saver.save(sess,model_path)\n",
    "        print(\"Saved model at: {}\".format(model_path))\n",
    "        #encoder_useful_state = sess.run(encoder_concat_everything)\n",
    "    return encoder_useful_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized session\n",
      "At epoch: 0\n",
      "At batch: 0\n",
      "Batch loss: 9.0673456192\n",
      "Epoch training loss: 8.67879604158\n",
      "Saved model at: ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    }
   ],
   "source": [
    "encoder_out = train_graph(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3665045 ,  0.19248164, -0.34810996, ..., -0.7770532 ,\n",
       "        -0.06400465,  0.28331846],\n",
       "       [ 0.35242832,  0.2059004 , -0.11128056, ..., -0.75132567,\n",
       "        -0.02789734,  0.3261328 ],\n",
       "       [ 0.38270092,  0.07817139, -0.59795696, ..., -0.6560076 ,\n",
       "        -0.08645746,  0.39262387],\n",
       "       ...,\n",
       "       [ 0.39830253,  0.04827934, -0.6380171 , ..., -0.5388174 ,\n",
       "        -0.07801261,  0.26077485],\n",
       "       [ 0.15671724,  0.04524764, -0.34900898, ..., -0.7905662 ,\n",
       "        -0.10922995,  0.37479112],\n",
       "       [ 0.48515686,  0.4221653 , -0.5161304 , ..., -0.52702236,\n",
       "        -0.04700104,  0.05832616]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e1de389c0cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_useful_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_concat_everything\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Cell to continue training'''\n",
    "with tf.Session(config=config) as sess:\n",
    "    #saver = tf.train.import_meta_graph('../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_100hiddenunits.meta')\n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_100hiddenunits.data-00000-of-00001'))\n",
    "    #sess.run()\n",
    "    saver.restore(sess, model_path)\n",
    "    batch_generator = gen_batch(idd_data, batch_size)\n",
    "    for batch in batch_generator:        \n",
    "        fd = next_feed(batch)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        encoder_useful_state = sess.run(encoder_concat_everything, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Cell to test in format for experiment'''\n",
    "def restored_model_enc_out(model_exists_already=True):\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(\"Initialized session\")\n",
    "        if(model_exists_already):\n",
    "            print(\"loading existing model\")\n",
    "            saver.restore(sess, model_path)\n",
    "        \n",
    "       \n",
    "        f_enc = feed_enc(holdout_data)\n",
    "        encoder_useful_state = sess.run(encoder_concat_everything, f_enc)\n",
    "            \n",
    "        #encoder_useful_state = sess.run(encoder_concat_everything)\n",
    "    return encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''Cell to test in format for experiment'''\n",
    "def restored_model_enc_out(model_exists_already=True):\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print(\"Initialized session\")\n",
    "        if(model_exists_already):\n",
    "            print(\"loading existing model\")\n",
    "            saver.restore(sess, model_path)\n",
    "        \n",
    "       \n",
    "        f_enc = (holdout_data)\n",
    "        encoder_useful_state = sess.run(encoder_concat_everything, f_enc)\n",
    "            \n",
    "        #encoder_useful_state = sess.run(encoder_concat_everything)\n",
    "    return encoder_useful_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized session\n",
      "loading existing model\n",
      "INFO:tensorflow:Restoring parameters from ../results/seq2seq/laptop_seq2seqmodel_embedsGlove_200d_50hiddenunits.ckpt\n"
     ]
    }
   ],
   "source": [
    "encoder_useful_state = restored_model_enc_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''INPUTS'''\n",
    "input_word_seq_tensor = tf.convert_to_tensor(np.array([[0,1,2,3,1,2],[9,10,11,12,13,12]]),dtype='int32')\n",
    "max_seq_length = 6\n",
    "enc_dims = 50\n",
    "seq_lengths = tf.convert_to_tensor(np.array([4,5]),dtype='int32') #shape n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Create mask based on max seq length'''\n",
    "np_mask_matrix = np.ones((max_seq_length,max_seq_length)) #num drops=n\n",
    "a = np.array(range(max_seq_length))\n",
    "np_mask_matrix[np.arange(len(a)), a] = 0\n",
    "#np_mask_matrix  = np_mask_matrix!=0\n",
    "tf_mask_matrix = tf.convert_to_tensor(np_mask_matrix, dtype=\"bool\")\n",
    "padding = tf.constant([[0,0],[0,1]],dtype='int32')\n",
    "resultant_tensor = tf.expand_dims(input_word_seq_tensor,0)\n",
    "#print(resultant_tensor.eval())\n",
    "tensor_seq_lengths = tf.expand_dims(seq_lengths, 0)\n",
    "seq_lengths_for_dropped = tf.expand_dims(seq_lengths-tf.ones(shape=seq_lengths.shape[0],dtype=\"int32\"),0)\n",
    "#print(seq_lengths_for_dropped)\n",
    "\n",
    "#def condition(x):\n",
    " #   x.shape[0]< max_seq_length\n",
    "#def body(x):\n",
    " #   Copy\n",
    "\n",
    "for drop_index in range(max_seq_length):\n",
    "    '''Create mask function-> note the drop index changes at each iteration'''\n",
    "    f = lambda word_seq: tf.boolean_mask(word_seq, tf_mask_matrix[:,drop_index])#,padding,\"CONSTANT\") \n",
    "    '''Apply masking and padding'''\n",
    "    #temp = tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\") #We apply mask and then pad result to enable concatenation with original \n",
    "    #temp = tf.expand_dims(temp,0)\n",
    "    resultant_tensor = tf.concat([resultant_tensor,tf.expand_dims(tf.pad(tf.map_fn(f, input_word_seq_tensor), padding, \"CONSTANT\"),0)],0)\n",
    "    tensor_seq_lengths = tf.concat([tensor_seq_lengths, seq_lengths_for_dropped],0)\n",
    "#print(resultant_tensor.eval())\n",
    "#print(resultant_tensor.shape)\n",
    "#print(tensor_seq_lengths.eval())\n",
    "#print(tensor_seq_lengths.shape)\n",
    "resultant_tensor = tf.reshape(resultant_tensor,[resultant_tensor.shape[0]*resultant_tensor.shape[1],resultant_tensor.shape[2]])\n",
    "tensor_seq_lengths = tf.reshape(tensor_seq_lengths,[tensor_seq_lengths.shape[0]*tensor_seq_lengths.shape[1],])\n",
    "#seq2seq_out = get_encoded_rep()\n",
    "#the seq2seq_out = reshaping of\n",
    "'''get_encoder_reps returns the batched encoder representations (shape = [(input_max_length+2)*(input_batch_size) , encoder_dims]'''\n",
    "#x = get_encoder_reps(resultant_tensor,tensor_seq_lengths,dims)\n",
    "seq2seq_out = tf.reshape(get_encoder_reps(resultant_tensor,tensor_seq_lengths), [max_seq_length+1, input_word_seq_tensor.shape[0], enc_dims])\n",
    "\n",
    "#Each column corresponds to the vector rep for a sentence\n",
    "#Each row corresponds to the word representation\n",
    "\n",
    "#We now reduce this by doing an op(row_0,row_i) for all columns\n",
    "'''NOTE : Hard coded here with subtract have to replace with op(row_0, row_i) for all rows i'''\n",
    "seq2seq_out = tf.subtract(seq2seq_out,seq2seq_out[0,:])[1:,:]\n",
    "#So final output should be max_seq_len*n_batches*dims --> do transpose if needed\n",
    "seq2seq_out = tf.transpose(seq2seq_out,perm=[1,0,2]) #convert to n_batches*max_seq_len*dims or n_batches*words*dims\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#apply function operation that takes first index and performs op(first_index, slice)\n",
    "#print(seq2seq_in.eval())\n",
    "#print(seq2seq_in.shape)\n",
    "#print(seq2seq_in_lengths.eval())\n",
    "#print(seq2seq_in_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"Initialized session\")\n",
    "        if(model_exists_already):\n",
    "            print(\"loading existing model\")\n",
    "            saver.restore(sess, model_path)\n",
    "        \n",
    "       \n",
    "        f_enc = (holdout_data)\n",
    "        encoder_useful_state = sess.run(encoder_concat_everything, f_enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
